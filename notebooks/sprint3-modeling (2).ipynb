{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90c8d27-d09c-426c-b5c5-0d90881815ac",
   "metadata": {},
   "source": [
    "# 1. Modeling Strategy & Implementation\n",
    "\n",
    "## 1.1 Model Selection Strategy\n",
    "\n",
    "### Target (y)\n",
    "- **unemployment_rate** (county-level continuous variable)\n",
    "\n",
    "### Features (X)\n",
    "- `foreign_born_pct`\n",
    "- `sanctuary_status` (1 = sanctuary, 0 = non-sanctuary)\n",
    "- (optional) poverty_rate, median_income, etc.\n",
    "\n",
    "### Why Regression?\n",
    "The target variable is numeric (unemployment_rate), so a **regression** approach is required.\n",
    "\n",
    "### Candidate Models\n",
    "1. **Linear Regression**  \n",
    "   - Baseline model  \n",
    "   - Highly interpretable  \n",
    "   - Good for understanding overall relationships  \n",
    "\n",
    "2. **Random Forest Regressor**  \n",
    "   - Captures interactions & non-linear patterns  \n",
    "   - Handles mixed feature types  \n",
    "   - Usually higher performance than linear models  \n",
    "\n",
    "3. **Gradient Boosting Regressor (Optional)**  \n",
    "   - Stronger ensemble model  \n",
    "   - Often leads to best performance  \n",
    "\n",
    "### Tradeoffs\n",
    "- Linear Regression → Simple, fast, interpretable  \n",
    "- Random Forest → Higher accuracy, less interpretable  \n",
    "- Gradient Boosting → Best accuracy, highest complexity  \n",
    "\n",
    "### Why these models?\n",
    "These models are widely used in policy and economics modeling and are appropriate for mixed demographic + policy datasets.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b562e-9f8a-477d-8752-853b4f20cea2",
   "metadata": {},
   "source": [
    "## 1.2 Hyperparameter & Design Decisions\n",
    "\n",
    "### Linear Regression\n",
    "- Using default sklearn settings\n",
    "- No hyperparameters to tune\n",
    "\n",
    "### Random Forest Regressor\n",
    "Key hyperparameters:\n",
    "- `n_estimators = 200`\n",
    "- `max_depth = None` (let forest grow fully)\n",
    "- `min_samples_split = 2`\n",
    "\n",
    "Reasoning:\n",
    "- Default parameters work for initial model testing  \n",
    "- Forest size increased slightly for stability  \n",
    "\n",
    "### Gradient Boosting (optional)\n",
    "- `n_estimators = 300`\n",
    "- `learning_rate = 0.05`\n",
    "- `max_depth = 3`\n",
    "\n",
    "Reasoning:\n",
    "- Lower learning rate → better generalization  \n",
    "- Shallow trees reduce overfitting  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b4e21-dd6c-4006-a7d8-c32b5da97a23",
   "metadata": {},
   "source": [
    "## 1.3 Data Splitting Strategy\n",
    "\n",
    "### Train/Test Split\n",
    "- `train_test_split(test_size=0.2, random_state=42)`\n",
    "- 80% training, 20% testing\n",
    "\n",
    "### Why?\n",
    "- Standard ratio for medium-size datasets  \n",
    "- Ensures enough data for training while keeping a representative test set  \n",
    "\n",
    "### Cross-Validation\n",
    "- Using **5-fold cross-validation** for Random Forest and Gradient Boosting  \n",
    "- Provides more stable performance estimates  \n",
    "- Helps avoid overfitting  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fbeabd-2c6f-4a3f-b054-079b9d05dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"../data/BLS_clean.csv\")\n",
    "\n",
    "# Select features\n",
    "X = df[[\"foreign_born_pct\", \"sanctuary_status\"]]\n",
    "y = df[\"unemployment_rate\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Baseline model\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "pred_lr = linreg.predict(X_test)\n",
    "\n",
    "# Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Optional Gradient Boosting\n",
    "gb = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "pred_gb = gb.predict(X_test)\n",
    "\n",
    "print(\"Linear Regression R²:\", r2_score(y_test, pred_lr))\n",
    "print(\"Random Forest R²:\", r2_score(y_test, pred_rf))\n",
    "print(\"Gradient Boosting R²:\", r2_score(y_test, pred_gb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd25f0-b212-46be-a27c-84e96fcbf5ff",
   "metadata": {},
   "source": [
    "1.2 Hyperparameter & Design Decisions\n",
    "\n",
    "Before training the models, I identified the key settings that control how each algorithm learns. Some hyperparameters were left at their default values (due to the small dataset), while others were selected based on best practices in regression tasks.\n",
    "\n",
    "1. Linear Regression\n",
    "\n",
    "No major hyperparameters.\n",
    "\n",
    "Chosen as a baseline because it is simple, fast, and highly interpretable.\n",
    "\n",
    "No tuning required.\n",
    "\n",
    "2. Random Forest Regressor\n",
    "\n",
    "Key hyperparameters considered:\n",
    "\n",
    "n_estimators (number of trees): starting with 100\n",
    "\n",
    "max_depth: controls level of tree splitting\n",
    "\n",
    "min_samples_split: minimum samples needed to split\n",
    "\n",
    "random_state: ensures reproducibility\n",
    "\n",
    "Design choice:\n",
    "For Sprint 3, I will begin with default parameters plus a small test adjustment to n_estimators (e.g., 100 → 200).\n",
    "A full GridSearch is possible in Sprint 4 but may be slow given the dataset size.\n",
    "\n",
    "3. Gradient Boosting Regressor (optional)\n",
    "\n",
    "Key hyperparameters:\n",
    "\n",
    "learning_rate\n",
    "\n",
    "n_estimators\n",
    "\n",
    "max_depth\n",
    "\n",
    "Design choice:\n",
    "Defaults will be used for Sprint 3 to avoid overfitting and because GBMs can be sensitive to tuning.\n",
    "If performance improves meaningfully, deeper tuning can be done in Sprint 4.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
